# to repro
- make a kind cluster
- docker exec into one of the nodes 
- `iptables-save > a` 
- make a new pod `kubectl scale deployment coredns -n kube-system --replicas=3`
- `iptables-save > b` <-- a second time.
- diff a b  <-- look at results

```
   Generated by iptables-save v1.8.3 on Tue Sep  8 13:26:56 2020
```

# MANGLE table
```
   *mangle
```
##  These are the "chains" in the mangle table (BUILT IN)
```
   :PREROUTING ACCEPT [168835:211614992]
   :INPUT ACCEPT [168835:211614992]
   :FORWARD ACCEPT [0:0]
   :OUTPUT ACCEPT [57983:14375471]
   :POSTROUTING ACCEPT [57983:14375471]
```

## These are the user defined (-N) chains...  i.e. by kube-proxy, todo, what do these do? 

These are made by kubelet...
```
   :KUBE-KUBELET-CANARY - [0:0]
   :KUBE-PROXY-CANARY - [0:0]
```

Now the commit command writes these out. `iptables COMMIT

```
   COMMIT
   # Completed on Tue Sep  8 13:26:56 2020
   # Generated by iptables-save v1.8.3 on Tue Sep  8 13:26:56 2020
```

# FILTER table

First we define all the chains for the filter table... 
Note that BOTH *Filter*, and *Mangle* have `INPUT`, `FORWARD`, and `OUTPUT` chains, wheras
the NAT table has `PREROUTING`, `POSTROUTING` as well.

Thus, the ONLY chain that ALL 3 TABLES have in commmon is, `OUTPUT`.

```
    *filter
    :INPUT ACCEPT [168700:211574950]
    :FORWARD ACCEPT [0:0]
    :OUTPUT ACCEPT [57856:14325161]
```

Again, like in the mangle table, we've got 6 kube created "chains"... 
```
    :KUBE-EXTERNAL-SERVICES - [0:0]
    :KUBE-FIREWALL - [0:0]
    :KUBE-FORWARD - [0:0]
    :KUBE-KUBELET-CANARY - [0:0]
    :KUBE-PROXY-CANARY - [0:0]
    :KUBE-SERVICES - [0:0]
``` 

Now we define the rules on the Filter table... note that all these rules are 
- effecting a built in chain (input/forward/output)
- jumping to a user chain... 
- as an example , you could do `-j DROP` instead of `-j KUBE-SERVICES` this would break all services.

```
    -A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
    -A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
    -A INPUT -j KUBE-FIREWALL
    -A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
    -A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
    -A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
    -A OUTPUT -j KUBE-FIREWALL
```

Now, lets look at the special Kubernetes chains for the Filter table.  the `-m mark --mark mark 0x8000`, results in "dropping" of packets.  You can see 
how this is used later on in the NAT table. See https://github.com/fwmark/registry as an example of an attempt to standardize all the marks
used by different firewalls/iptables tools.

```
    -A KUBE-FIREWALL -m comment --comment "kubernetes firewall for dropping marked packets" -m mark --mark 0x8000/0x8000 -j DROP
```

Now, we make sure that INVALID conntrack connections are immediately dropped.  We trust conntrack that, if it invalidated a packet, then TCP transaction is done.
```
    -A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP
```

Similar to the above, we use 0x4000 as the mark for ACCEPTING traffic.   This is traffic that has been masked earlier on.  See the NAT table.
```
    -A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
```

Continuing with the accpetance of ongoing conntrack connctions...  
```
    -A KUBE-FORWARD -m comment --comment "kubernetes forwarding conntrack pod source rule" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
    -A KUBE-FORWARD -m comment --comment "kubernetes forwarding conntrack pod destination rule" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
```

In this case, kube-dns had no endpoints up yet at the time this cluster was running.... A good exercise here would be to scale down coredns, and see if
in a live cluster, you see this rule existing or not.

```
    -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment "kube-system/kube-dns:dns has no endpoints" -m udp --dport 53 -j REJECT --reject-with icmp-port-unreachable
    -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:dns-tcp has no endpoints" -m tcp --dport 53 -j REJECT --reject-with icmp-port-unreachable
    -A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment "kube-system/kube-dns:metrics has no endpoints" -m tcp --dport 9153 -j REJECT --reject-with icmp-port-unreachable
    COMMIT
```

# NAT table

Note this is the hardest part... so the notes here are very raw.  But we'll improve them later.  the NAT table is the most heavily used by K8s, because it has the dynamic rules in it for KUBE-SEP and KUBE-SVC.

This table *actually mutates* the packet fields.  This is distinct from MANGLE table, bc the mangle table, is additive.  Note this is a semantic collision w/ the mangle table.  Thats because, NAT'ing IS mangling ~ i.e. it it is mutating packet fields.  However in IPTABLES.
- The MANGLE table ONLY changes QOS, Address.  This happens FIRST.
- The NAT table ONLY changes the parts of the packet related to routing.  This happens AFTER the Mangle table.

This is because the STRUCTURE of a packet is ordered like this:

`Version,IHL,TOS,TTL,.... Source Address, Destination Address, Options, Data...`.  To look at an example packet, see https://en.wikipedia.org/wiki/IPv4#/media/File:IPv4_Packet_-en.svg.

```
# Completed on Tue Sep  8 13:26:56 2020
# Generated by iptables-save v1.8.3 on Tue Sep  8 13:26:56 2020
*nat
```

First we look at the nat table chains... 
```
  :PREROUTING ACCEPT [17:1020]
  :INPUT ACCEPT [17:1020]
  :OUTPUT ACCEPT [125:7824]
  :POSTROUTING ACCEPT [193:12336]
```
In this table we have a bunch of chaings INCLUDING dynamically created chains, i.e. i.e. see  
- KUBE-SVC-...
- KUBE-SEP-...
```
  :DOCKER_OUTPUT - [0:0]
  :DOCKER_POSTROUTING - [0:0]
  :KUBE-KUBELET-CANARY - [0:0]
  :KUBE-MARK-DROP - [0:0]
  :KUBE-MARK-MASQ - [0:0]
  :KUBE-NODEPORTS - [0:0]
  :KUBE-POSTROUTING - [0:0]
  :KUBE-PROXY-CANARY - [0:0]
  :KUBE-SEP-OJCTP5LCEHQJ3D72 - [0:0]
  :KUBE-SERVICES - [0:0]
  :KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]
```

Now we can look at the rules for the NAT chains.  
- PREROUTING rules are the packet, before it's 
```
  # 0) before anything we start here... jump to kube service
  -A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
```
Ignore these DOCKER_OUTPUT rules, they are related to some complexity in Kind. 
See https://github.com/kubernetes-sigs/kind/blame/main/images/base/files/usr/local/bin/entrypoint#L344 "Patch dockers iptables rules
to switch out the DNS IP".
```
  -A PREROUTING -d 192.168.65.2/32 -j DOCKER_OUTPUT
  -A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
  -A OUTPUT -d 192.168.65.2/32 -j DOCKER_OUTPUT
```
 
We use Kube-postrouting as the way that we post-route ALL traffic from the NAT table.  this is where ultimately masquerading happens from pod source traffic.  All egress traffic needs to be post-routed with the Nodes ip.  

TODO, clearly, the NAT table is somehow friends with conntrack.  Need to ask anotonio how ?

```  
  -A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
```

As above, another docker accidental complexity..  https://github.com/moby/libnetwork/blob/master/resolver_unix.go for the DOCKER_POSTROUTING rules.
Just like DOCKER_OUTPUT these arent related to Kubernetes specifically... 

```
-A POSTROUTING -d 192.168.65.2/32 -j DOCKER_POSTROUTING
  -A DOCKER_OUTPUT -d 192.168.65.2/32 -p tcp -m tcp --dport 53 -j DNAT --to-destination 127.0.0.11:36637
  -A DOCKER_OUTPUT -d 192.168.65.2/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:34367
  -A DOCKER_POSTROUTING -s 127.0.0.11/32 -p tcp -m tcp --sport 36637 -j SNAT --to-source 192.168.65.2:53
  -A DOCKER_POSTROUTING -s 127.0.0.11/32 -p udp -m udp --sport 34367 -j SNAT --to-source 192.168.65.2:53
```

Remember that bit we parsed earlier (0x8000).  This is where it was set.  Remember that with iptables, you a RULE can TARGET(jump), either to

- to a chain
- to an action

```  
  -A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
```

Its really weird that when you jump, you dont know wether or not your going to a jain or an action.

We're almost done defining our NAT rules.  Now, we're going to define a masq bit on the packet.
We'll use this bit if we EVER get to the `KUBE-MARK-MASQ` chain.... which we will in a few lines...
```
# This is where we masq traffic that will be read in the FILTER table to forward traffic.
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x4000/0x4000 -j MASQUERADE --random-fully
```

Ok, so, here is where we use that 0x4000 bit.  We will jump to the KUBE-MARK-MASQ chain if someone is from the node, (172.18.0.5) then we jump to the KUBE-MARK-MASQ chain.  
```
## ... finally hit the service endpoint. , and masq it IF WE ARE local..... OTHERWISE go to 
-A KUBE-SEP-OJCTP5LCEHQJ3D72 -s 172.18.0.5/32 -m comment --comment "default/kubernetes:https" -j KUBE-MARK-MASQ

## ... this is the alternative to 5a, i.e. if the source is not localhost we DNAT.... TODO, look at the DNAT rules for KUBE-SEP and see what they do.... 
-A KUBE-SEP-OJCTP5LCEHQJ3D72 -p tcp -m comment --comment "default/kubernetes:https" -m tcp -j DNAT --to-destination 172.18.0.5:6443
```

```
# 1) not in the pod IP CIDR, and headed to an k8s default api service , jump to  MASQ the packet  
-A KUBE-SERVICES ! -s 192.168.0.0/16 -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-MARK-MASQ
```

```
# 3) jump back from the masq, here, and now we finally hit the actual k8s *service* 
-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
# 4) hitting the NPX service.... jump to an ENDPOINT this would fractional if we had more then one endpoint i.e. --probability .3... 
-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https" -j KUBE-SEP-OJCTP5LCEHQJ3D72
COMMIT
```
